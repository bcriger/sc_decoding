"""
We're going to calculate the probability that a certain coset is
correct, given a syndrome, by brute force.
This is a subroutine for RG decoding, and it can be used to generate
training data for the NN.  
"""

import numpy as np, itertools as it, circuit_metric as cm
import sparse_pauli as sp
from functools import reduce
from operator import mul, or_ as union, xor as sym_diff
from scipy.special import betainc
import circuit_metric.SCLayoutClass as SCLayoutClass
import decoding_2d as dc

#from the itertools cookbook
def powerset(iterable):
    "powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
    s = list(iterable)
    return it.chain.from_iterable(it.combinations(s, r) for r in range(len(s)+1))

def weight_dist(stab_gens, identity, log, coset_rep, ltr='x'):
    """
    If we iterate over an entire stabiliser group, we can calculate
    the weight of each Pauli of the form c * l * s for c a coset
    representative for the normaliser of the stabiliser, l a choice of 
    logical operator, and s a stabiliser.
    Colloquially, c is generated by a one-to-one map from the
    syndromes.

    The weight will range from 0 to nq, where nq is the number of
    qubits in the system, and there'll be many Paulis with each weight.
    Therefore, we return an array full of counts.

    For speed, I do the set sym_diffs directly   
    """

    # First get nq
    nq = len(reduce(union, (s.support() for s in stab_gens)))

    wt_counts_I = np.zeros((nq + 1,), dtype=np.int_)
    wt_counts_L = np.zeros((nq + 1,), dtype=np.int_)
    
    # i = 0
    # TODO Strip single set and do sym_diff instead of mul

    # for subset in powerset(stab_gens):
    #     i += 1
    #     s = reduce(mul, subset, sp.Pauli())
    #     wt_counts_I[(s * identity * coset_rep).weight()] += 1
    #     wt_counts_L[(s * log * coset_rep).weight()] += 1
    #     #if i % 1000 == 0:
    #     #    print(i)
    
    ltr = ltr.lower()
    if ltr == 'x':
        bare_sets = [gen.x_set for gen in stab_gens]
        coset_rep_set = coset_rep.x_set
        log_coset_rep_set = log.x_set ^ coset_rep.x_set
    elif ltr == 'z':
        bare_sets = [gen.z_set for gen in stab_gens]
        coset_rep_set = coset_rep.z_set
        log_coset_rep_set = log.z_set ^ coset_rep.z_set
    
    for subset in powerset(bare_sets):
        s = reduce(sym_diff, subset, set())
        wt_counts_I[len(s ^ coset_rep_set)] += 1
        wt_counts_L[len(s ^ log_coset_rep_set)] += 1

    return [wt_counts_I, wt_counts_L]

def prob_integral(weight_counts, p_lo, p_hi):
    """
    The probability of a logical out given a syndrome coset rep in is
    sum_{s in stab_group} (p/(1-p))^{|c * l * s|} where c is the coset
    rep, and l is the logical.
    We're going to sample p over a uniform distribution from p_lo to
    p_hi, so we calculate the expected value of this probability over
    the distribution.
    """
    n = len(weight_counts) - 1
    return sum([
                    c * (betainc(w + 1, n - w + 1, p_hi) 
                        - betainc(w + 1, n - w + 1, p_lo))
                    for w, c in enumerate(weight_counts)
                ])/(p_hi - p_lo)

def single_prob(weight_counts, p):
    """
    For debugging purposes, I'd like to have a function that evaluates
    the coset probability at a single point in p-space, so that I can
    see whether the integral is off. This is going to get normalised
    anyway, so we can output probabilities that are off by an overall
    factor of (1 - p) ** n.
    """

    return [sum([ c * (p / (1. - p)) ** w for w, c in enumerate(weight_counts[0])]), 
	        sum([ c * (p / (1. - p)) ** w for w, c in enumerate(weight_counts[1])])]

def coset_prob(stab_gens, log, coset_rep, p_lo, p_hi):
    return prob_integral(weight_dist(stab_gens, log, coset_rep), p_lo, p_hi)

if __name__ == '__main__':
    distance = 5
    x_anc_len = 12
    #test_layout = cm.SCLayoutClass.SCLayout(5)
    test_layout = SCLayoutClass.SCLayout(distance)
    x_stabs = list(test_layout.stabilisers()['X'].values())
    log = test_layout.logicals()[0]
    sim_test = dc.Sim2D(distance, distance, 0.01) # probability is irrelevant since we provide directly the error
    lst = []
    z_ancs_keys = list(sim_test.layout.z_ancs())
    cycles = 0
    store_s = []
    while len(lst) < 500:
        cycles += 1
        rnd_err = sim_test.random_error()
        #print(rnd_err)
        synds = sim_test.syndromes(rnd_err)
        #print(synds[1])
        dumb_x_corr, dumb_z_corr = sim_test.dumb_correction(synds)
        coset_rep = dumb_x_corr     #sp.Pauli({5,13,22,26,33,42,46},{})
        prob_dist = single_prob(weight_dist(x_stabs, sp.Pauli(), log, coset_rep), 0.01)
        #single_prob(weight_dist(x_stabs, coset_rep), 0.01)       
        norm = sum(prob_dist)
        prob_dist = [p / norm for p in prob_dist]
        
        list_z = [0] * len(z_ancs_keys)
        for k in synds[1]:
            key = sim_test.layout.map.inv[k]
            pos = z_ancs_keys.index(key)
            list_z[pos] = 1
        
        s = ''
        for i in range(x_anc_len):
            s += str(list_z[i])
		
        if s not in store_s:
            store_s.append(s)
            lst.append([s, prob_dist])

        if cycles % 20 == 0:
            print(len(lst), cycles)
            #print('store_s ', store_s)
    
    for i in range(len(lst)):
        print(lst[i])

#---------------------------------------------------------------------#
