"""
We're going to calculate the probability that a certain coset is
correct, given a syndrome, by brute force.
This is a subroutine for RG decoding, and it can be used to generate
training data for the NN.  
"""
#I hope, one day, that I don't have to do this anymore:
from sys import version_info
if version_info[0] == 3:
    import pickle as pkl #You have to change this for python 3.
elif version_info[0] == 2:
    import cPickle as pkl
else:
    raise ImportError("Can't understand python version: "
                        "{}".format(version_info))

from copy import deepcopy
import circuit_metric as cm
#import circuit_metric.SCLayoutClass as SCLayoutClass
import SCLayoutClass
import decoding_2d as dc2
import decoding_3d as dc3
import error_model as em
from functools import reduce
import itertools as it
import numpy as np
from operator import mul, or_ as union, xor as sym_diff
from run_script import fancy_dist
from scipy.special import betainc
import sparse_pauli as sp


#from the itertools cookbook
def powerset(iterable):
    "powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
    s = list(iterable)
    return it.chain.from_iterable(it.combinations(s, r) for r in range(len(s)+1))

def weight_dist(stab_gens, identity, log, coset_rep, ltr='x'):
    """
    If we iterate over an entire stabiliser group, we can calculate
    the weight of each Pauli of the form c * l * s for c a coset
    representative for the normaliser of the stabiliser, l a choice of 
    logical operator, and s a stabiliser.
    Colloquially, c is generated by a one-to-one map from the
    syndromes.

    The weight will range from 0 to nq, where nq is the number of
    qubits in the system, and there'll be many Paulis with each weight.
    Therefore, we return an array full of counts.

    For speed, I do the set sym_diffs directly   
    """

    # First get nq
    nq = len(reduce(union, (s.support() for s in stab_gens)))

    wt_counts_I = np.zeros((nq + 1,), dtype=np.int_)
    wt_counts_L = np.zeros((nq + 1,), dtype=np.int_)
    # i = 0
    
    # for subset in powerset(stab_gens):
    #     i += 1
    #     s = reduce(mul, subset, sp.Pauli())
    #     wt_counts_I[(s * identity * coset_rep).weight()] += 1
    #     wt_counts_L[(s * log * coset_rep).weight()] += 1
    #     #if i % 1000 == 0:
    #     #    print(i)
    
    ltr = ltr.lower()
    if ltr == 'x':
        bare_sets = [gen.x_set for gen in stab_gens]
        coset_rep_set = coset_rep.x_set
        log_coset_rep_set = log.x_set ^ coset_rep.x_set
    elif ltr == 'z':
        bare_sets = [gen.z_set for gen in stab_gens]
        coset_rep_set = coset_rep.z_set
        log_coset_rep_set = log.z_set ^ coset_rep.z_set
    
    for subset in powerset(bare_sets):
        s = reduce(sym_diff, subset, set())
        wt_counts_I[len(s ^ coset_rep_set)] += 1
        wt_counts_L[len(s ^ log_coset_rep_set)] += 1

    return [wt_counts_I, wt_counts_L]

def full_weight_dist(stab_gens, log_x, log_z, coset_rep):
    """
    I revert to full Pauli calculations in order to look at
    depolarizing errors.
    """

    # First get nq
    nq = len(reduce(union, (s.support() for s in stab_gens)))
    wt_counts = {
            'I': np.zeros((nq + 1,), dtype=np.int_),
            'X': np.zeros((nq + 1,), dtype=np.int_),
            'Y': np.zeros((nq + 1,), dtype=np.int_),
            'Z': np.zeros((nq + 1,), dtype=np.int_)
            }
    coset_rep_X = coset_rep * log_x
    coset_rep_Y = coset_rep * log_x * log_z
    coset_rep_Z = coset_rep * log_z
    
    for subset in powerset(stab_gens):
        s = reduce(mul, subset, sp.Pauli())
        wt_counts['I'][(s * coset_rep).weight()] += 1
        wt_counts['X'][(s * coset_rep_X).weight()] += 1
        wt_counts['Y'][(s * coset_rep_Y).weight()] += 1
        wt_counts['Z'][(s * coset_rep_Z).weight()] += 1

    return wt_counts

def prob_integral(weight_counts, p_lo, p_hi):
    """
    The probability of a logical out given a syndrome coset rep in is
    sum_{s in stab_group} (p/(1-p))^{|c * l * s|} where c is the coset
    rep, and l is the logical.
    We're going to sample p over a uniform distribution from p_lo to
    p_hi, so we calculate the expected value of this probability over
    the distribution.

    DOES NOT WORK
    """
    n = len(weight_counts) - 1
    return sum([
                    c * (betainc(w + 1, n - w + 1, p_hi) 
                        - betainc(w + 1, n - w + 1, p_lo))
                    for w, c in enumerate(weight_counts)
                ])/(p_hi - p_lo)

cnt2p = lambda cnt, p: sum([ c * (p / (1. - p)) ** w for w, c in enumerate(cnt)])

def single_prob(weight_counts, p):
    """
    For debugging purposes, I'd like to have a function that evaluates
    the coset probability at a single point in p-space, so that I can
    see whether the integral is off. This is going to get normalised
    anyway, so we can output probabilities that are off by an overall
    factor of (1 - p) ** n.

    Note: they're not off by that number, they're off by p(synd).
    """
    p_vec = map(cnt2p, weight_counts)
    norm = sum(p_vec)
    return [p / norm for p in p_vec]

def dep_prob(weight_count_dict, dep_p):
    """
    Does the same thing as single_prob, except that now I'm trying to
    correct depolarization.
    I return four probabilities [p_I, p_X, p_Y, p_Z].
    """
    p_lst = []
    for ki in 'IXYZ':
        p_lst.append(cnt2p(weight_count_dict[ki], dep_p / 3.))
    norm = sum(p_lst)
    return [p / norm for p in p_lst]

def coset_prob(stab_gens, log, coset_rep, p_lo, p_hi):
    return prob_integral(weight_dist(stab_gens, log, coset_rep), p_lo, p_hi)

def unique_list():
    distance = 5
    x_anc_len = 12
    test_layout = SCLayoutClass.SCLayout(distance)
    x_stabs = list(test_layout.stabilisers()['X'].values())
    log = test_layout.logicals()[0]
    sim_test = dc.Sim2D(distance, distance, 0.01) # probability is irrelevant since we provide directly the error
    lst = []
    z_ancs_keys = list(sim_test.layout.z_ancs())
    cycles = 0
    store_s = []
    while len(lst) < 500:
        cycles += 1
        rnd_err = sim_test.random_error()
        #print(rnd_err)
        synds = sim_test.syndromes(rnd_err)
        #print(synds[1])
        dumb_x_corr, dumb_z_corr = sim_test.dumb_correction(synds)
        coset_rep = dumb_x_corr     #sp.Pauli({5,13,22,26,33,42,46},{})
        prob_dist = single_prob(weight_dist(x_stabs, sp.Pauli(), log, coset_rep), 0.01)
        
        list_z = [0] * len(z_ancs_keys)
        for k in synds[1]:
            key = sim_test.layout.map.inv[k]
            pos = z_ancs_keys.index(key)
            list_z[pos] = 1
        
        s = ''
        for i in range(x_anc_len):
            s += str(list_z[i])
        
        if s not in store_s:
            store_s.append(s)
            lst.append([s, prob_dist])

        if cycles % 20 == 0:
            print(len(lst), cycles)
            #print('store_s ', store_s)
    
    for i in range(len(lst)):
        print(lst[i])

    #return lst

def dist_5_check():
    """
    runs weight-1 and 2 errors for a dist-5 SC
    """
    #test_layout = cm.SCLayoutClass.SCLayout(5)
    test_layout = SCLayoutClass.SCLayout(5)
    x_stabs = list(test_layout.stabilisers()['X'].values())
    ds = [test_layout.map[d] for d in test_layout.datas]
    x_errs = [sp.Pauli({d}, set()) for d in ds]
    x_errs += [sp.Pauli(pr, set()) for pr in it.combinations(ds,r=2)]
    log = test_layout.logicals()[0]
    for err in x_errs:
        prob_dist = single_prob(weight_dist(x_stabs, err, log, err), 0.01)
        if prob_dist[0] < 2.: #dummy
            print(err, prob_dist)

def dist_7_single_sample():
    #test_layout = cm.SCLayoutClass.SCLayout(7)
    test_layout = SCLayoutClass.SCLayout(7)
    x_stabs = list(test_layout.stabilisers()['X'].values())
    ds = [test_layout.map[d] for d in test_layout.datas]
    x_errs = [sp.Pauli({d}, set()) for d in ds]
    x_errs += [sp.Pauli(pr, set()) for pr in it.combinations(ds,r=2)]
    log = test_layout.logicals()[0]
    prob_dist = single_prob(weight_dist(x_stabs, x_errs[52], log, x_errs[52]), 0.01)

def dist_5_tabulate():
    """
    I only intend to run this function once, it's going to pickle all
    the weight_dist's for a distance-5 code correcting x errors.
    """
    anc_lst = [0, 1, 8, 10, 18, 20, 28, 30, 38, 40, 47, 48]
    sim_5 = dc.Sim2D(5, 5, 0.01)
    log_x, log_z = sim_5.layout.logicals()
    x_stabs = list(sim_5.layout.stabilisers()['X'].values())
    weight_dict = {}
    blsm_cosets = {}
    for subset in powerset(anc_lst):
        coset_rep = sim_5.dumb_correction([[], list(subset)], origin=True)[0]
        blsm_corr = sim_5.graphAndCorrection(subset, 'X')
        key = synd_to_bits(subset, anc_lst)
        val = weight_dist(x_stabs, sp.Pauli(), log_x, coset_rep)
        weight_dict[key] = val
        blsm_cosets[key] = (blsm_corr * coset_rep).com(log_z)
    
    with open('weight_counts_5.pkl', 'w') as phil:
        pkl.dump(weight_dict, phil)
    
    with open('blsm_cosets_5.pkl', 'w') as phil:
        pkl.dump(blsm_cosets, phil)

def dist_5_fancy_tabulate():
    """
    I want to know if Tom's weights produce disagreement for
    independent X/Z decoding @ distance 5.

    Checking at p = 0.01, there are no different syndromes when using
    fancy/non-fancy weights
    """
    anc_lst = [0, 1, 8, 10, 18, 20, 28, 30, 38, 40, 47, 48]
    sim_5 = dc.Sim2D(5, 5, 0.01, useBlossom=False)
    log_x, log_z = sim_5.layout.logicals()
    x_stabs = list(sim_5.layout.stabilisers()['X'].values())
    weight_dict = {}
    blsm_cosets = {}
    d_func = fancy_dist(5, 0.01) #test for other values of p?
    for subset in powerset(anc_lst):
        synd = [[],list(subset)]
        coset_rep = sim_5.dumb_correction(synd, origin=True)[0]
        g = sim_5.graph(subset, dist_func=d_func)
        blsm_corr = sim_5.correction(g, 'X')
        key = synd_to_bits(subset, anc_lst)
        blsm_cosets[key] = (blsm_corr * coset_rep).com(log_z)
    return blsm_cosets


def dist_3_corr_tabulate():
    """
    Mostly a copypasta of dist_5_tabulate, but I'm trying for a
    depolarizing error model.
    """
    sim_3 = dc.Sim2D(3, 3, 0.00)
    sim_3.error_model = em.depolarize(0.03,
                    [[sim_3.layout.map[_]] for _ in sim_3.layout.datas])
    x_anc_lst = sorted([sim_3.layout.map[crd] for crd in sim_3.layout.x_ancs()])
    z_anc_lst = sorted([sim_3.layout.map[crd] for crd in sim_3.layout.z_ancs()])
    anc_lst = sorted(x_anc_lst + z_anc_lst)
    log_x, log_z = sim_3.layout.logicals()
    stab_gens = sum(map(lambda x: x.values(),
                        sim_3.layout.stabilisers().values()), [])

    synds = it.product(powerset(x_anc_lst), powerset(z_anc_lst))
    output_dict = {}
    blsm_cosets = {}
    for x_synd, z_synd in synds:
        coset_rep = reduce(mul, sim_3.dumb_correction([x_synd, z_synd], origin=True))
        key = synd_to_bits(x_synd + z_synd, anc_lst)
        val = full_weight_dist(stab_gens, log_x, log_z, coset_rep)
        x_corr = sim_3.graphAndCorrection(x_synd, 'Z')
        z_corr = sim_3.graphAndCorrection(z_synd, 'X')
        loop = x_corr * z_corr * coset_rep 
        if loop.com(log_z):
            blsm_cosets[key] = 'Y' if loop.com(log_x) else 'X'
        else:
            blsm_cosets[key] = 'Z' if loop.com(log_x) else 'I'
        output_dict[key] = val
    
    with open('corr_wts_3.pkl', 'w') as phil:
        pkl.dump(output_dict, phil)
    
    with open('corr_cosets_3.pkl', 'w') as phil:
        pkl.dump(blsm_cosets, phil)
    
    pass

def dist_5_log_error_rate(p_arr):
    """
    Using the weights we counted immediately above, I evaluate:
    sum_{syndrome} p(syndrome) * p(fail | syndrome).

    p(fail | syndrome) is the minimum of single_prob(wts, p), 
    """
    n = 25.
    with open('weight_counts_5.pkl', 'r') as phil:
        wc_dict = pkl.load(phil)
    
    with open('blsm_cosets_5.pkl', 'r') as phil:
        bc_dict = pkl.load(phil)
    
    log_ps = np.zeros(p_arr.shape)
    blsm_log_ps = np.zeros(p_arr.shape)
    for idx, p in enumerate(p_arr):
        for synd, weight_counts in wc_dict.items():
            p_vec = [sum([ c * (p / (1. - p)) ** w for w, c in enumerate(weight_counts[0])]), 
                    sum([ c * (p / (1. - p)) ** w  for w, c in enumerate(weight_counts[1])])]
            log_ps[idx] += min(p_vec)
            blsm_log_ps[idx] += p_vec[1 - bc_dict[synd]]
        log_ps[idx] *= (1. - p) ** n
        blsm_log_ps[idx] *= (1. - p) ** n
    
    return log_ps, blsm_log_ps

def dumbest_dataset(d, p, total_sz, flnm=None):
    """
    Tries the dumbest possible technique, making a bunch of random
    samples so we can evaluate the gradient without any hassle.
    p: float (between 0 and 0.5, to make sense it should be ~0.05)
    total_sz: combined number of training, validation and test instances.
    """
    if not(d % 2):
        raise ValueError("doesn't work for even d")
    n_synds = int((d**2 - 1) / 2)
    sim = dc.Sim2D(d, d, p)
    input_arr = np.zeros((total_sz, n_synds))
    label_arr = np.zeros(total_sz, dtype=np.int_)
    _, log_z = sim.layout.logicals()
    ancs = sorted([sim.layout.map[anc] for anc in sim.layout.z_ancs()])
    
    for idx in range(total_sz):
        err = sim.random_error()
        x_synd, z_synd = sim.syndromes(err)
        #x errors only -> z syndromes only
        input_arr[idx, :] = map(float, synd_to_bits(z_synd, ancs))
        #does correcting to the origin result in a logical?
        dumb_x_corr, _ = sim.dumb_correction((x_synd, z_synd), True)
        label_arr[idx] = (dumb_x_corr * err).com(log_z)
    
    if flnm is None:
        return {"input": input_arr, "labels": label_arr}
    else:
        with open(flnm, 'w') as phil:
            pkl.dump({"input": input_arr, "labels": label_arr}, phil)
        pass

def dist_3_corr_log_error_rate(p_arr):
    """
    For a depolarizing error model, I evaluate the different
    probabilities of logical error (X, Y, or Z)
    """
    pass

def calc_parity_3d(dist, meas_rnds, model, use_blossom):
    """
    Calculates the amount of errors after d faulty Surface 
    code rounds mod 2. Based on the result we know the logical
    state of the logical qubit
    """
    #dq = [1,2,3,7,8,9,13,14,15]
    no_anc = (d + 1) * (d - 1) / 2 
    flp = [0] * no_anc
    #flips_X = flp * (d+1)
    flips_Z = [0] * (d+1)
    for i in range(d+1):
        flips_Z[i] = deepcopy(flp)
    
    sim = dc3.Sim3D(dist, meas_rnds, model, use_blossom)
    err_history, syndrome_history = sim.history()
    #print('err_history ', err_history)
    #print('syndrome_history ', syndrome_history)
    #print("-----------")
    #print(err_history[2].x_set)
    #print(err_history[2].z_set)
    #print("-----------")
    #print(len(err_history[2].x_set))
    #print(len(err_history[2].z_set))
    #print("-----------")
    #print(len(err_history[2].x_set) % 2)
    #print(len(err_history[2].z_set) % 2)
    #print("-----------")
    x_parity = len(err_history[2].x_set) % 2
    #z_parity = len(err_history[2].z_set) % 2
    
    if x_parity == 1:
        log_err = 'X'
    else:
        log_err = 'I'
        
    #if x_parity == 0:
    #    log_err = 'I' if z_parity == 0 else 'Z'
    #else:
    #    log_err = 'X' if z_parity == 0 else 'Y'
    #print('log_err ', log_err)
    #print(syndrome_history.items())
    
    #sim_test = dc.Sim2D(3, 3, 0.01)
    #test_layout = SCLayoutClass.SCLayout(3)
    #print(sim_test.layout.map)
    #print(sim_test.layout.map.inv)
    #x_ancs = sim_test.layout.x_ancs()
    #z_ancs = sim_test.layout.z_ancs()
    #errors_X = ''
    #errors_Z = ''
    #for i in range(len(dq)):
    #    errors_X += ('1' if dq[i] in err_history[2].x_set else '0') + ' '
    #    errors_Z += ('1' if dq[i] in err_history[2].z_set else '0') + ' '
    
    #if d == 3:
    #x_ancs = [4,6,10,12]
    #z_ancs = [0,5,11,16]
    #elif d == 5:
    #x_ancs = [7,9,11,17,19,21,27,29,31,37,39,41]
    z_ancs = [0,1,8,10,18,20,28,30,38,40,47,48]
    
    '''
    for j in range(4):
        for i in range(4):
            if (x_ancs[i] in syndrome_history['X'][j] and x_ancs[i] in syndrome_history['X'][j+1]) or \
               (x_ancs[i] not in syndrome_history['X'][j] and x_ancs[i] not in syndrome_history['X'][j+1]):
                flips_X[j][i] = 0
            else:
                flips_X[j][i] = 1
    '''
    #print(flips_Z)
    for j in range(d+1):
        for i in range(no_anc):
            if (z_ancs[i] in syndrome_history['Z'][j] and z_ancs[i] in syndrome_history['Z'][j+1]) or \
               (z_ancs[i] not in syndrome_history['Z'][j] and z_ancs[i] not in syndrome_history['Z'][j+1]):
                flips_Z[j][i] = 0
            else:
                flips_Z[j][i] = 1
    
    return flips_Z, log_err   # flips_X, [errors_X, errors_Z]#, [x_parity, z_parity]
#-------------------------convenience functions-----------------------#
def synd_to_bits(synd, loc_list):
    """
    You put in a sparse int list (idxs where a check is violated) and 
    a (sorted) list of the possible locations where a check could be
    violated, and this function returns a string of ones and zeros.
    """
    return ''.join(['1' if _ in synd else '0' for _ in loc_list])

def bits_to_synd(bits, loc_list):
    """
    You put in a sparse int list (idxs where a check is violated) and 
    a (sorted) list of the possible locations where a check could be
    violated, and this function returns a string of ones and zeros.
    """
    return [l for l, b in zip(loc_list, bits) if b == 1]

#---------------------------------------------------------------------#

if __name__ == '__main__':
    # unique_list()
    # dist_5_check()
    # dist_5_tabulate()
    # p_arr = np.linspace(0.001,0.5,50)
    # test_vec = dist_5_log_error_rate(p_arr)
    # dist_7_single_sample()
    #dumbest_dataset(5, 0.08, 1000000, 'd_5_dataset_million')
    d = 3
    ms_rnd = 3
    no_anc = int((d + 1) * (d - 1) / 2 )
    p = 0.04
    q = 0.04
    samples = {}
    z_ancs = [0,5,11,16]
    flp = [0] * no_anc
    flips_Z = [0] * (d+1)
    for i in range(d+1):
        flips_Z[i] = deepcopy(flp)
    no_samples = 10000
    sim_test2d = dc2.Sim2D(d, d, p)
    sim_test = dc3.Sim3D(d, d, ('pq', p, p), True)
    #err = sim_test.run(no_samples, progress=False, metric=None, bdy_info=None, final_perfect_rnd=True)
    #print(err)
    
    #calc_parity_3d(dist, meas_rnds, model, use_blossom)
    
    for cycl in range(1000000):
        err, synd = sim_test.history(True)
        rnd_err = err[2].x_set
        rd_er = sp.Pauli(rnd_err,[])
        corr_blossom = sim_test.correction(synd, metric=None, bdy_info=None)
        mwpm_log_err = sim_test.logical_error(err[-1], corr_blossom)
        synds_2d = sim_test2d.syndromes(rd_er)
        dumb_x_corr, dumb_z_corr = sim_test2d.dumb_correction(synds_2d, False)
        dumb_log_err = sim_test2d.logical_error(rd_er, dumb_x_corr, dumb_z_corr)
        
        for j in range(d+1):
            for i in range(no_anc):
                if (z_ancs[i] in synd['Z'][j] and z_ancs[i] in synd['Z'][j+1]) or \
                   (z_ancs[i] not in synd['Z'][j] and z_ancs[i] not in synd['Z'][j+1]):
                    flips_Z[j][i] = 0
                else:
                    flips_Z[j][i] = 1
        
        s = ''
        for j in range(d+1):
            for k in range(no_anc):
                s += str(flips_Z[j][k])
        
        if s in samples:
            x = samples[s]
            if mwpm_log_err == 'X' or mwpm_log_err == 'Y':
                if dumb_log_err == 'I' or dumb_log_err == 'Z':
                    samples[s] = [x[0], x[1]+1, x[2]+1, x[3], x[4]+1]
                elif dumb_log_err == 'X' or dumb_log_err == 'Y':
                    samples[s] = [x[0], x[1]+1, x[2], x[3]+1, x[4]+1]
            else:
                if dumb_log_err == 'I' or dumb_log_err == 'Z':
                    samples[s] = [x[0]+1, x[1], x[2]+1, x[3], x[4]+1]
                elif dumb_log_err == 'X' or dumb_log_err == 'Y':
                    samples[s] = [x[0]+1, x[1], x[2], x[3]+1, x[4]+1]
        else:
            if mwpm_log_err == 'X' or mwpm_log_err == 'Y':
                if dumb_log_err == 'I' or dumb_log_err == 'Z':
                    samples[s] = [0,1, 1,0, 1]
                else:
                    samples[s] = [0,1, 0,1, 1]
            elif dumb_log_err == 'X' or dumb_log_err == 'Y':
                if dumb_log_err != 'X' or dumb_log_err != 'Y':
                    samples[s] = [1,0, 1,0, 1]
                else:
                    samples[s] = [1,0, 0,1, 1]
        cycl += 1            
        if cycl % 1000 == 0:
            print(len(samples), cycl)

    print(len(samples))
    
    sorted_samples = sorted(samples.items(), key=lambda e: e[1][4], reverse=True)
    with open('comparison.txt', 'w') as f:    
        for key, value in sorted_samples:
            x = ''
            for i in range((d+1) * no_anc):
                x += key[i] + ' '

            x += ' '
            x += str(value[0]) + ' '
            x += str(value[1]) + ' '
            x += str(value[2]) + ' '
            x += str(value[3]) + ' '
            x += str(value[4]) + '\n'
            f.write(x)

    '''
    for i in range(10000000):
        flips_Z, log_err = calc_parity_3d(d, ms_rnd, ('pq', p, q), True)
        #print(flips_Z)    
        s = ''
        #print(d+1, no_anc)
        for j in range(d+1):
            for k in range(no_anc):
                s += str(flips_Z[j][k])

        if s in samples:
            x = samples[s]
            samples[s] = [x[0], x[1]+1]
        else:
            if log_err == 'X' or log_err == 'Y':
                samples[s] = [[0,1], 1]		
            else:
                samples[s] = [[1,0], 1]
                
        if i % 1000 == 0:
            print(len(samples), i)

    print(len(samples))
    sorted_samples = sorted(samples.items(), key=lambda e: e[1][1], reverse=True)
    with open('d='+str(d)+'_p='+str(p)+'_3D_'+str(len(samples))+'_samples.txt', 'w') as f:
        for key, value in sorted_samples:
            x = ''
            for i in range((d+1) * no_anc):
                x += key[i] + ' '

            x += ' '
            x += str(value[0][0]) + ' '
            x += str(value[0][1]) + ' '
            x += str(value[1]) + '\n'
            f.write(x)

        #print('flips_X ', flips_X)
        #print('flips_Z ', flips_Z)
        #print('log_err ', log_err)
        #print('---------')
    '''